{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63381861-5a27-49ca-8d63-d31d8fcabd8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 04_train_mlflow (fixed)\n",
    "# Train a RandomForest on churn features, log to MLflow, and (attempt) register model.\n",
    "# Use this corrected cell — it avoids restricted dbutils calls in CE.\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "# Config (safe for CE)\n",
    "# --------------------------\n",
    "FEATURES_TABLE = \"churn_mlo_mdb.features_churn\"\n",
    "# Use a simple, constant experiment name — avoid restricted dbutils calls\n",
    "EXPERIMENT_NAME = \"churn_experiment\"\n",
    "MODEL_REGISTRY_NAME = \"ChurnModel\"\n",
    "SAMPLE_IF_TOO_BIG = True         # if toPandas would be too large, fallback to sampling\n",
    "MAX_ROWS_FOR_PANDAS = 200000     # safe threshold (CE limited), adjust down if memory errors\n",
    "\n",
    "# --------------------------\n",
    "# Load features table\n",
    "# --------------------------\n",
    "print(\"Reading feature table:\", FEATURES_TABLE)\n",
    "spark_df = spark.table(FEATURES_TABLE)\n",
    "count_rows = spark_df.count()\n",
    "print(\"Rows in features table:\", count_rows)\n",
    "spark_df.printSchema()\n",
    "\n",
    "# Convert to pandas safely\n",
    "def spark_to_pandas_safe(sdf, max_rows=MAX_ROWS_FOR_PANDAS):\n",
    "    n = sdf.count()\n",
    "    if n <= max_rows:\n",
    "        print(f\"Loading full table as pandas ({n} rows)\")\n",
    "        return sdf.toPandas()\n",
    "    else:\n",
    "        print(f\"Table too large ({n} rows). Sampling up to {max_rows} rows (approx).\")\n",
    "        frac = float(max_rows) / float(n)\n",
    "        # approximate sample; for a more exact stratified sample we would compute fractions per class\n",
    "        pdf = sdf.sample(withReplacement=False, fraction=frac, seed=42).toPandas()\n",
    "        print(\"Sampled rows:\", len(pdf))\n",
    "        return pdf\n",
    "\n",
    "pdf = spark_to_pandas_safe(spark_df)\n",
    "\n",
    "# Basic check\n",
    "print(\"Pandas dataframe shape:\", pdf.shape)\n",
    "print(\"Columns:\", pdf.columns.tolist())\n",
    "\n",
    "# --------------------------\n",
    "# Prepare X, y\n",
    "# --------------------------\n",
    "if \"customerID\" in pdf.columns:\n",
    "    customer_ids = pdf[\"customerID\"].astype(str).tolist()\n",
    "    pdf = pdf.drop(columns=[\"customerID\"])\n",
    "\n",
    "# Ensure churn_label exists\n",
    "if \"churn_label\" not in pdf.columns:\n",
    "    raise RuntimeError(\"churn_label not found in feature table. Please ensure silver/features step created it.\")\n",
    "\n",
    "# Fill missing numeric values (simple strategy) and ensure numeric types\n",
    "pdf = pdf.copy()\n",
    "pdf = pdf.fillna(0)\n",
    "\n",
    "# select numeric columns only for this simple demo\n",
    "numeric_df = pdf.select_dtypes(include=[np.number]).copy()\n",
    "X = numeric_df.drop(columns=[\"churn_label\"], errors='ignore')\n",
    "y = numeric_df[\"churn_label\"].astype(int)\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \"Label shape:\", y.shape)\n",
    "\n",
    "# split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "print(\"Train/test sizes:\", X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "# --------------------------\n",
    "# MLflow experiment setup (safe)\n",
    "# --------------------------\n",
    "try:\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    print(\"MLflow experiment set to:\", EXPERIMENT_NAME)\n",
    "except Exception as e:\n",
    "    print(\"Warning: could not set experiment via API; continuing with default. Error:\", e)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# --------------------------\n",
    "# Hyperparameter search + training\n",
    "# --------------------------\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100],\n",
    "    \"max_depth\": [6, 8, 12],\n",
    "    \"min_samples_leaf\": [1, 2]\n",
    "}\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=1)\n",
    "\n",
    "grid = GridSearchCV(rf, param_grid, scoring=\"roc_auc\", cv=cv, verbose=1, n_jobs=1)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id\n",
    "    print(\"MLflow Run ID:\", run_id)\n",
    "    # Fit grid search\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    print(\"Best params:\", grid.best_params_)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    preds = best_model.predict(X_test)\n",
    "    probs = best_model.predict_proba(X_test)[:,1]\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"Test Accuracy: {acc:.4f}, ROC AUC: {auc:.4f}\")\n",
    "    print(classification_report(y_test, preds, digits=4))\n",
    "    \n",
    "    # Log parameters & metrics\n",
    "    mlflow.log_param(\"grid_params\", json.dumps(param_grid))\n",
    "    mlflow.log_param(\"best_params\", json.dumps(grid.best_params_))\n",
    "    mlflow.log_metric(\"test_accuracy\", float(acc))\n",
    "    mlflow.log_metric(\"test_roc_auc\", float(auc))\n",
    "    mlflow.log_metric(\"train_rows\", int(X_train.shape[0]))\n",
    "    mlflow.log_metric(\"test_rows\", int(X_test.shape[0]))\n",
    "    \n",
    "    # Save feature names as artifact\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    with open(\"/tmp/feature_names.json\", \"w\") as f:\n",
    "        json.dump(feature_names, f)\n",
    "    mlflow.log_artifact(\"/tmp/feature_names.json\", artifact_path=\"features\")\n",
    "    \n",
    "    # Log feature importances as artifact & table\n",
    "    importances = best_model.feature_importances_\n",
    "    fi = pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
    "    fi = fi.sort_values(\"importance\", ascending=False)\n",
    "    fi.to_csv(\"/tmp/feature_importances.csv\", index=False)\n",
    "    mlflow.log_artifact(\"/tmp/feature_importances.csv\", artifact_path=\"features\")\n",
    "    \n",
    "    # Log the model and attempt to register in model registry\n",
    "    try:\n",
    "        mlflow.sklearn.log_model(best_model, artifact_path=\"model\", registered_model_name=MODEL_REGISTRY_NAME)\n",
    "        print(\"Model logged and registration attempted for:\", MODEL_REGISTRY_NAME)\n",
    "    except Exception as e:\n",
    "        # fallback: log without auto-registration\n",
    "        print(\"Auto-register failed (CE sometimes restricts). Logging model artifact only. Error:\", e)\n",
    "        mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n",
    "    \n",
    "    # Save a local copy (optional)\n",
    "    joblib.dump(best_model, \"/tmp/best_model.joblib\")\n",
    "    mlflow.log_artifact(\"/tmp/best_model.joblib\", artifact_path=\"model_files\")\n",
    "    \n",
    "    # Tag the run with feature count & feature list length (useful for monitoring)\n",
    "    try:\n",
    "        mlflow.set_tag(\"n_features\", str(len(feature_names)))\n",
    "        mlflow.set_tag(\"feature_list\", \",\".join(feature_names))\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    print(\"Run finished. Run ID:\", run_id)\n",
    "\n",
    "# --------------------------\n",
    "# Post-run: check model registry / print registered versions\n",
    "# --------------------------\n",
    "try:\n",
    "    versions = client.get_latest_versions(MODEL_REGISTRY_NAME)\n",
    "    if versions:\n",
    "        print(\"Latest model registry versions for\", MODEL_REGISTRY_NAME)\n",
    "        for v in versions:\n",
    "            print(\"Version:\", v.version, \"Stage:\", v.current_stage, \"RunID:\", v.run_id)\n",
    "    else:\n",
    "        print(\"No versions found in registry (maybe auto-register failed).\")\n",
    "except Exception as e:\n",
    "    print(\"Could not query model registry. Error:\", e)\n",
    "\n",
    "# Show artifact URI for the run\n",
    "try:\n",
    "    run_info = client.get_run(run_id)\n",
    "    artifact_uri = run_info.info.artifact_uri\n",
    "    print(\"Artifact URI for run:\", artifact_uri)\n",
    "except Exception as e:\n",
    "    print(\"Could not fetch run details. Error:\", e)\n",
    "\n",
    "# Save a small metrics summary in /tmp and log it\n",
    "try:\n",
    "    metrics_summary = {\"accuracy\": acc, \"roc_auc\": auc, \"n_train\": int(X_train.shape[0]), \"n_test\": int(X_test.shape[0])}\n",
    "    with open(\"/tmp/metrics_summary.json\", \"w\") as f:\n",
    "        json.dump(metrics_summary, f)\n",
    "    mlflow.log_artifact(\"/tmp/metrics_summary.json\", artifact_path=\"metrics\")\n",
    "except Exception as e:\n",
    "    print(\"Could not save metrics artifact:\", e)\n",
    "\n",
    "print(\"Training cell completed. Check the MLflow experiment UI for run details and the Models UI for registry.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f722bf31-91c8-4a3d-9fd2-50a7077d2912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_train_mlflow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
